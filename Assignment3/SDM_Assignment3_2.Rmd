---
title: "SDM_Assignment3_2"
author: "Sri Balaji Muruganandam"
date: "25/10/2021"
output: html_document
---

### Setting Working Directory
```{r}
rm(list = ls())
setwd("G:\\SDM_Sem01\\Assignment3")
```

### Importing necessary libraries
```{r}
library(ISLR)
library(caret)
library(class)
library(MASS)
```

### Viewing the Sample data
```{r}
head(Weekly, 5)
dim(Weekly)
```

## a)  Produce  some  numerical  and  graphical  summaries  of  the  “Weekly”  data.    Do there  appear  to  be  any  patterns?
### Exploring the high level overview of the data
```{r}
str(Weekly)
summary(Weekly)
```

### The values are uniformly distributed over the year
```{r}
hist(Weekly$Year,breaks = 20, col="#2F5D62", border = "#5E8B7E", main = "Year is uniformly distributed",xlab = "No of data", ylab = "Year")
```

### Distribution of Volume
```{r}
hist(Weekly$Volume,breaks = 12, col="#334257", border = "#476072", main = "Distribution of Volume",xlab = "Volume", ylab = "Frequency")
```


### There is a positive correlation between Volume and the Year
```{r}
plot(Weekly$Volume, Weekly$Year,col="#00918E", xlab = "Volume", ylab = "Year", ylim = c(1990,2012), main = "Correlation of Volume and the Year", sub="They have positive correlation", lwd = 2.3)
table(Weekly$Direction)
```

### Lags are normally distributed
```{r}
par(mfrow = c(3,2),bg = '#EEEEEE')
hist(Weekly$Lag1,breaks = 25, col="#4B3869", border = "#6D8299", main = "Histogram of Lag1", xlab = "Lag1", ylab = "Frequency")
hist(Weekly$Lag2,breaks = 25, col="#4B3869", border = "#6D8299", main = "Histogram of Lag2", xlab = "Lag2", ylab = "Frequency")
hist(Weekly$Lag3,breaks = 25, col="#4B3869", border = "#6D8299", main = "Histogram of Lag3", xlab = "Lag3", ylab = "Frequency")
hist(Weekly$Lag4,breaks = 25, col="#4B3869", border = "#6D8299", main = "Histogram of Lag4", xlab = "Lag4", ylab = "Frequency")
hist(Weekly$Lag5,breaks = 25, col="#4B3869", border = "#6D8299", main = "Histogram of Lag5", xlab = "Lag5", ylab = "Frequency")
```

## b)  Use  the  full  data  to  perform  logistic  regression  with  “Direction”  as  the  response and  the  five  lag  variables,  plus  volume,  as  predictors.    Use  the  summary function  to  print  the  results.    Do  any  of  the  predictors  appear  to  be  statistically significant?    Comment  on  these. 

### Splitting the data into training and the test data
```{r}
set.seed(23)
random_index = sample(c(1:nrow(Weekly)), size = round(8/10 * nrow(Weekly)), replace = FALSE)
data1 = Weekly[,-c(1,8)]
train_data <- data1[random_index,]
test_data <- data1[-random_index,]
train_data = data.frame(train_data)
test_data = data.frame(test_data)
data1 = data.frame(data1)

y_train_data <- as.numeric(train_data$Direction)-1
y_test_data <- as.numeric(test_data$Direction)-1
dim(train_data)
dim(test_data)
```
### Modelling Logistic Regression
```{r}
model = glm(Direction~., data = data1, family = "binomial")
summary(model)
```

### The predictor Lag2 has a coefficient of 0.05844 and seems to be significant for classification

## c)  Compute  the  “confusion  matrix”  and  overall  fraction  of  correct  predictions.   Explain  what  the  confusion  matrix  is  telling  you  about  the  types  of  mistakes made  by  logistic  regression. 

### Prediction for train data
```{r}
train_predict = predict(model, newdata = train_data, type = "response")
y_predict_train = round(train_predict)
```

### Predicting for new values
```{r}
test_predict = predict(model, newdata = test_data, type = "response")
y_predict_test = round(test_predict)
```

### Calculating the error
```{r}
train_error <- sum(abs(y_predict_train- y_train_data))/length(y_train_data)
test_error <- sum(abs(y_predict_test- y_test_data))/length(y_test_data)
print(train_error)
print(test_error)
```

### Computing the confusion matrix
```{r}
conf <- confusionMatrix(as.factor(y_predict_test), as.factor(y_test_data))
names(conf)
conf$table
conf$overall
```

### From the result it is shown that 112 UP's and 8 DOWN's are predicted well.
### Whereas 8 UP's are incorrectly predicted as DOWN's and 90 DOWN's are incorrectly predicted as UP's
### Accuracy of the model is 5.504587e-01

### Calculating the accuracy using formula
```{r}
false_result = which(y_predict_test==y_test_data)
accuracy=length(false_result) / length(y_test_data)
print(accuracy)
```

## d)  Fit  the  logistic  model  using  a  training  data  period  from  1990-2008,  with  “Lag2” as  the  only  predictor.    Compute  the  confusion  matrix,  and  the  overall  correct fraction  of  predictions  for  the  held  out  data  (that  is,  the  data  from  2009  and 2010).

### Creating new dataset for the training and testing data based on the new conditions
```{r}
data2 = Weekly[,c(1,3,9)]
random_index = which(data2$Year %in% c(1990:2008))
knn_index = Weekly$Year <= 2008
train_data2 <- data2[random_index,]
test_data2 <- data2[-random_index,]

knn_train = Weekly[knn_index,"Lag2",drop=F]
knn_test = Weekly[knn_index,"Lag2",drop=F]
knn_y_train = Weekly[knn_index,"Direction",drop=T]
knn_y_test = Weekly[knn_index,"Direction",drop=T]

train_data2 = data.frame(train_data2[,-c(1)])
test_data2 = data.frame(test_data2[,-c(1)])
data2 = data.frame(data2[,-c(1)])


y_train_data2 <- as.numeric(train_data2$Direction)-1
y_test_data2 <- as.numeric(test_data2$Direction)-1
dim(train_data2)
dim(test_data2)
```
```{r}
head(data2,4)
head(train_data2,2)
head(test_data2,2)
```

### Modelling Logistic Regression with the newly generated data
```{r}
model2 = glm(Direction~., data = data2, family = "binomial")
summary(model2)
```

### Prediction for train data
```{r}
train_predict2 = predict(model2, newdata = train_data2, type = "response")
y_predict_train2 = round(train_predict2)
```

### Predicting for new values
```{r}
test_predict2 = predict(model2, newdata = test_data2, type = "response")
y_predict_test2 = round(test_predict2)
```

### Calculating the error
```{r}
train_error2 <- sum(abs(y_predict_train2- y_train_data2))/length(y_train_data2)
test_error2 <- sum(abs(y_predict_test2- y_test_data2))/length(y_test_data2)
print(train_error2)
print(test_error2)
```
### With the newly generated data, the training error is 0.4446701 and the testing error is 0.375

### Computing the confusion matrix
```{r}
conf2 <- confusionMatrix(as.factor(y_predict_test2), as.factor(y_test_data2))
names(conf2)
conf2$table
conf2$overall
```

### From the result it is shown that 56 UP's and 9 DOWN's are predicted well.
### Whereas 5 UP's are incorrectly predicted as DOWN's and 34 DOWN's are incorrectly predicted as UP's
### Accuracy of the model is 6.250000e-01

## e)  Repeat  (d)  using  LDA.
### Modelling LDA
```{r}
lda_model = lda(Direction~., data = train_data2)
summary(lda_model)
```

### Plotting the values of LDA
```{r}
plot(lda_model)
```

### Predicting for test data
```{r}
lda_predict_train = predict(lda_model, newdata = train_data2)
lda_predict_test = predict(lda_model, newdata = test_data2)
res_train = as.numeric(lda_predict_train$class)-1
res_test = as.numeric(lda_predict_test$class)-1
```
### Calculating the train and test errors
```{r}

lda_result_train = which(res_train==y_train_data2)
lda_train_error=length(lda_result_train) / length(y_train_data2)
print(lda_train_error)


lda_result_test = which(res_test==y_test_data2)
lda_test_error=length(lda_result_test) / length(y_test_data2)
print(lda_test_error)
```

### The train error is 0.5543147 and the test error is 0.625 when predicted using LDA.

## f)  Repeat  (d)  using  KNN  with  k=1.
### Predicting using KNN with k = 1
```{r}
set.seed(23)
knn_model = knn(knn_train,knn_test,knn_y_train,k=1)
```

### Calculating Error
```{r}
error_knn<- mean(knn_model != knn_y_test)
print(error_knn)
```

### Calculating Accuracy
```{r}
false_result_knn = which(knn_model == knn_y_test)
knn_accuracy = length(false_result_knn)/length(knn_y_test)
print(knn_accuracy)
```

## g)  Which  method  appears  to  provide  the  best  results? 
### KNN model provides the best prediction with an accuracy of 0.9593909. Other models provide less accuracy than KNN model.

## h)  Experiment  with  different  combinations  of  predictors,  including  possible transformations  and  interactions,  for  each  method.    Report  the  variables, method,  and  associated  confusion  matrix  that  appears  to  provide  the  best results  on  the  held-out  data.    Note  that  you  should  also  experiment  with  values for  K  in  the  kNN  classifier.

## Performing Logistic Regression
### Now considering only the return in week 1, 2 and 3
### Prediction for train data
```{r}
model_test = glm(Direction~Lag1+Lag2+Lag3, data = data1, family = "binomial")
train_predict = predict(model_test, newdata = train_data, type = "response")
y_predict_train = round(train_predict)
```

### Predicting for new values
```{r}
test_predict = predict(model, newdata = test_data, type = "response")
y_predict_test = round(test_predict)
```

### Calculating the error
```{r}
train_error <- sum(abs(y_predict_train- y_train_data))/length(y_train_data)
test_error <- sum(abs(y_predict_test- y_test_data))/length(y_test_data)
print(train_error)
print(test_error)
```

### Computing the confusion matrix
```{r}
conf2 <- confusionMatrix(as.factor(y_predict_test), as.factor(y_test_data))
names(conf2)
conf2$table
conf2$overall
```
## The train error is 0.445465 and the test error is 0.4495413 for the above model

## Modelling LDA
## Considering only the columns Lag2, Lag4 Lag5 along with the Volume
```{r}
lda_model = lda(Direction~Lag2+Lag4+Lag5+Volume, data = train_data)
summary(lda_model)
```

### Predicting for test data
```{r}
lda_predict_train = predict(lda_model, newdata = train_data)
lda_predict_test = predict(lda_model, newdata = test_data)
res_train = as.numeric(lda_predict_train$class)-1
res_test = as.numeric(lda_predict_test$class)-1
```
### Calculating the train and test errors
```{r}

lda_result_train = which(res_train==y_train_data)
lda_train_error=length(lda_result_train) / length(y_train_data)
print(lda_train_error)


lda_result_test = which(res_test==y_test_data)
lda_test_error=length(lda_result_test) / length(y_test_data)
print(lda_test_error)
```

### The train error is 0.5579793 and the test error is 0.5550459 when predicted using LDA.

## Modelling KNN with various k values (k = 3,5,7,9,11,13,15)
```{r}
kvalues = c(3,5,7,9,11,13,15)
error_knn = c()
knn_accuracy = c()
for(i in 1:7) {
  predict_knn <- knn(knn_train,knn_test,knn_y_train,k=i)
  error_knn[i]<- mean(predict_knn != knn_y_test)
  
  false_result_knn = which(predict_knn == knn_y_test)
  knn_accuracy[i] = length(false_result_knn)/length(knn_y_test)
}
```

### Error at each k value
```{r}
print(error_knn)
print(min(error_knn))
```

### Accuracy at each k value
```{r}
print(knn_accuracy)
print(max(knn_accuracy))
```

### Plotting K and the error value corresponding to each k value
```{r}
par(bg = '#EEEEEE')
plot(kvalues,error_knn, col="#E05D5D", type = "b", xlab = "K Value", ylab = "Test Error", ylim = c(0,0.5), main = "Test Error vs K", sub="Test Error is lower at k = 3", lwd = 3.0)
```

### The error is low when the value of k =3